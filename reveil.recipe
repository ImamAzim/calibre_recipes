#!/usr/bin/env python
# vim:fileencoding=utf-8

from calibre.web.feeds.news import BasicNewsRecipe
import re
import datetime

import dateutil.parser
from bs4 import BeautifulSoup

class AdvancedUserRecipe1667902828(BasicNewsRecipe):
    title          = 'reveil'
    language = 'fr'

    index_url = 'https://www.courrierinternational.com/feed/all/rss.xml'
    feeds_url = list()
    rubriques = [
            'france',
            'geopolitique',
            'economie',
            'societe',
            'politique',
            'science-environnement',
            'culture',
            ]
    for rubrique in rubriques:
        feed_url = f'https://www.courrierinternational.com/feed/rubrique/{rubrique}/rss.xml'
        feeds_url.append(feed_url)
    deadline_hour = 6
    deadline_minute = 35
    no_stylesheets = True
    auto_cleanup   = False
    max_articles_per_feed = 100
    une_du_jour_day = 3
    une_du_jour_name = 'a-la-une-de-l-hebdo'

    deadline = datetime.time(deadline_hour, deadline_minute)

    keep_only_tags = [
        dict(name='article', attrs={'class': 'article'}),
    ]
    remove_tags = [
        dict(name='aside', attrs={'class': 'article-aside'}),
        dict(name='div', attrs={'class': 'article-tags'}),
        dict(name='div', attrs={'class': 'article-readmore'}),
        dict(name='aside', attrs={'class': 'article-outbrain'}),
        dict(name='section', attrs={'class': 'article-recommandations'}),
        dict(name='section', attrs={'class': 'ci-services services-left'}),
    ]
    remove_attributes = [
            'loading',
            ]

    needs_subscription = True
    login_url = 'https://www.courrierinternational.com/login'

    def get_browser(self):
        def is_form_login(form):
            return "id" in form.attrs and form.attrs['id'] == "user-login-form"
        br = BasicNewsRecipe.get_browser(self)
        if self.username:
            br.open(self.login_url)
            br.select_form(predicate=is_form_login)
            br['name'] = self.username
            br['pass'] = self.password
            br.submit()
        return br

    def parse_index(self):
        raw_soup = self.index_to_soup(self.index_url, raw=True)
        soup = BeautifulSoup(raw_soup, 'xml')

        articles = list()

        for div in soup.find_all('item'):
            date_text=div.find('pubDate').text.strip()[0:-6]
            article_datetime = dateutil.parser.parse(date_text)
            if article_datetime.date() == datetime.date.today():
                if article_datetime.time() <= self.deadline:
                    title = div.title.text
                    description=div.find('description').text
                    url=div.find('link').text
                    article = dict(
                            title=title,
                            url=url,
                            date=date_text,
                            description=description,
                            )
                    articles.append(article)
        for feed_url in self.feeds_url:
            raw_soup = self.index_to_soup(feed_url, raw=True)
            soup = BeautifulSoup(raw_soup, 'xml')
            for div in soup.find_all('item'):
                url=div.find('link').text
                if self.une_du_jour_name in url:
                    date_text=div.find('pubDate').text.strip()[0:-6]
                    title = div.title.text
                    description=div.find('description').text
                    article = dict(
                            title=title,
                            url=url,
                            date=date_text,
                            description=description,
                            )
                    articles.append(article)
        feeds = [('Articles', articles)]
        return feeds


